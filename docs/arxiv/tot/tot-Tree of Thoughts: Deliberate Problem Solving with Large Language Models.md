はい、承知いたしました。PDFファイル「2305.10601v2.pdf」の全文を日本語に翻訳します。技術的な内容を含みますが、可能な限り自然な日本語になるよう努めます。ページごとに区切って翻訳を進めます。

-----

**ページ 1**

[source: 1] arXiv:2305.10601v2 [cs.CL] 2023年12月3日

**Tree of Thoughts: 大規模言語モデルを用いた意図的な問題解決**

Shunyu Yao
プリンストン大学

Thomas L. Griffiths
プリンストン大学

Dian Yu
Google DeepMind

Jeffrey Zhao
Google DeepMind

Izhak Shafran
Google DeepMind

Karthik Narasimhan
プリンストン大学

Yuan Cao
Google DeepMind

**要旨**

言語モデルは、広範なタスクにわたる一般的な問題解決のためにますます展開されていますが、推論時には依然としてトークンレベルの左から右への意思決定プロセスに限定されています。[source: 2] これは、探索、戦略的な先読み、または初期の決定が極めて重要な役割を果たすタスクにおいて、言語モデルが不十分である可能性があることを意味します。[source: 3] これらの課題を克服するために、我々は言語モデル推論のための新しいフレームワーク「Tree of Thoughts」（ToT）を導入します。これは、言語モデルをプロンプトする一般的な「Chain of Thought」（思考の連鎖）アプローチを一般化し、問題解決に向けた中間ステップとして機能する一貫したテキスト単位（「思考」）に対する探索を可能にします。[source: 4] ToTは、LM（大規模言語モデル）が複数の異なる推論経路を検討し、選択肢を自己評価して次に行うべき行動を決定することによって、意図的な意思決定を実行することを可能にします。また、必要に応じて先読みや後戻りを行い、全体的な選択を行うことも可能です。[source: 5] 我々の実験は、ToTが、自明ではない計画や探索を必要とする3つの新しいタスク（ゲーム・オブ・24、クリエイティブ・ライティング、ミニクロスワード）において、言語モデルの問題解決能力を大幅に向上させることを示しています。[source: 6] 例えば、ゲーム・オブ・24において、chain-of-thoughtプロンプティングを用いたGPT-4はタスクの4%しか解決できませんでしたが、我々の手法は74%の成功率を達成しました。[source: 7] 全てのプロンプトを含むコードリポジトリ： [https://github.com/princeton-nlp/tree-of-thought-1lm](https://www.google.com/search?q=https://github.com/princeton-nlp/tree-of-thought-1lm)。

**1 はじめに**

元々テキスト生成のために設計された、GPT [25, 26, 1, 23] や PaLM [5] のようなスケールアップされた言語モデル（LM）は、数学的、記号的、常識的、知識推論を必要とするますます広範なタスクを実行できることが示されています。[source: 8] おそらく驚くべきことに、この進歩の根底にあるのは、依然としてテキスト生成のための元の自己回帰メカニズムであり、これはトークンレベルの決定を一つずつ、左から右へと行います。[source: 9] このような単純なメカニズムは、LMが一般的な問題解決者として構築されるのに十分なのでしょうか？[source: 10] もしそうでなければ、どのような問題が現在のパラダイムに挑戦し、代替メカニズムは何であるべきでしょうか？

[source: 11] 人間の認知に関する文献は、これらの問いに答えるためのいくつかの手がかりを提供しています。[source: 12] 「二重プロセス」モデルに関する研究は、人々が意思決定に関与する際に2つのモードを持っていることを示唆しています - 速く、自動的で、無意識的なモード（「システム1」）と、遅く、意図的で、意識的なモード（「システム2」）です [30, 31, 16, 15]。[source: 13] これら2つのモードは、以前から機械学習で使用されるさまざまな数理モデルに関連付けられてきました。[source: 14] 例えば、人間や他の動物における強化学習に関する研究は、彼らが連想的な「モデルフリー」学習に従事する状況や、より意図的な「モデルベース」計画に従事する状況を探求してきました [7]。[source: 15] LMの単純な連想的なトークンレベルの選択も「システム1」を彷彿とさせ、したがって、より意図的な「システム2」計画プロセスによって補強されることで恩恵を受ける可能性があります。このプロセスは、（1）現在の選択肢に対して単一のものを選ぶだけでなく、多様な代替案を維持し探索し、

第37回 Neural Information Processing Systems 会議 (NeurIPS 2023).

-----

**ページ 2**

[Image 1] 画像1：様々なLLMを用いた問題解決アプローチを示す概略図。

[source: 16] 入力 入力 入力 入力 思考 出力 出力 (a) 入力-出力 (c) Chain of Thought プロンプティング (IO) プロンプティング (CoT) 多数決 出力 出力 (c) Self Consistency with CoT (COT-SC) (d) Tree of Thoughts (ToT)

図1：LLMを用いた問題解決への様々なアプローチを示す概略図。[source: 17] 各矩形ボックスは思考を表し、これは問題解決に向けた中間ステップとして機能する一貫した言語シーケンスです。[source: 18] 思考がどのように生成され、評価され、探索されるかの具体例については、図2、4、6を参照してください。[source: 19] (2) 現在の状態を評価し、積極的に先読みまたは後戻りして、より全体的な決定を下します。

[source: 20] このような計画プロセスを設計するために、我々は人工知能（および認知科学）の起源に立ち返り、ニューウェル、ショー、サイモンが1950年代から探求した計画プロセスからインスピレーションを得ます [21, 22]。[source: 21] ニューウェルらは、問題解決[21]を、組み合わせ的な問題空間を通じた探索として特徴づけました。これは木として表現されます。[source: 22] したがって、我々は言語モデルを用いた一般的な問題解決のためのTree of Thoughts (ToT) フレームワークを提案します。[source: 23] 図1が示すように、既存の手法（詳細は後述）が問題解決のために連続的な言語シーケンスをサンプリングするのに対し、ToTは思考の木を積極的に維持します。ここで各思考は、問題解決に向けた中間ステップとして機能する一貫した言語シーケンスです（表1）。[source: 24] このような高レベルのセマンティックユニットにより、LMは、言語でもインスタンス化される意図的な推論プロセスを通じて、異なる中間思考が問題解決に向けてどれだけ進展するかを自己評価できます（図2、4、6）。[source: 25] LMの自己評価と熟考による探索ヒューリスティクスのこの実装は新しいものです。なぜなら、以前の探索ヒューリスティクスはプログラムされるか学習されるかのいずれかだったからです。[source: 26] 最後に、我々はこの言語ベースの能力（多様な思考を生成し評価する）を、幅優先探索（BFS）や深さ優先探索（DFS）などの探索アルゴリズムと組み合わせます。これにより、先読みと後戻りを伴う思考の木の体系的な探索が可能になります。

[source: 27] 経験的に、我々は、最先端の言語モデルであるGPT-4 [23] を用いても既存のLM推論手法に挑戦する3つの新しい問題を提案します：ゲーム・オブ・24、クリエイティブ・ライティング、クロスワード（表1）。[source: 28] これらのタスクは、演繹的、数学的、常識的、語彙的な推論能力、および体系的な計画や探索を取り入れる方法を必要とします。[source: 29] 我々は、ToTが、異なるレベルの思考、思考を生成・評価する異なる方法、および異なる問題の性質に適応する異なる探索アルゴリズムをサポートするのに十分一般的かつ柔軟であることにより、3つのタスクすべてで優れた結果を得ることを示します。[source: 30] また、そのような選択がモデルのパフォーマンスにどのように影響するかを体系的なアブレーション（機能除去実験）を通じて分析し、LMをより良く訓練し使用するための将来の方向性について議論します。

-----

**ページ 3**

[source: 31] **2 背景**

まず、我々のアプローチが触発され、後に比較される、大規模言語モデルを用いた問題解決のためのいくつかの既存の手法を形式化します。[source: 32] パラメータ $\\theta$ を持つ事前訓練済みLMを $p\_{\\theta}$ で示し、小文字 $x, y, z, s, \\dots$ で言語シーケンス、すなわち $x = (x[1], \\dots, x[n])$ （各 $x[i]$ はトークン）を示します。これにより $p\_{\\theta}(x) = \\prod\_{i=1}^{n} p\_{\\theta}(x[i]|x[1\\dots i-1])$ となります。大文字 $S, \\dots$ で言語シーケンスの集合を示します。

[source: 33] 入力-出力（IO）プロンプティングは、問題入力 $x$ をLMで出力 $y$ に変換する最も一般的な方法です： $y \\sim p\_{\\theta}(y|prompt\_{IO}(x))$。ここで $prompt\_{IO}(x)$ は、タスク指示および/または少数ショットの入出力例で入力をラップします。[source: 34] 簡単のため、$p\_{\\theta}^{prompt}(\\text{output} | \\text{input}) = p\_{\\theta}(\\text{output} | \\text{prompt}(\\text{input}))$ と表記し、IOプロンプティングを $y \\sim p\_{\\theta}^{IO}(y|x)$ と定式化できるようにします。

[source: 36] Chain-of-thought（CoT）プロンプティング [38] は、入力 $x$ から出力 $y$ へのマッピングが自明でない場合（例えば、$x$ が数学の問題で $y$ が最終的な数値回答の場合）に対処するために提案されました。[source: 37] 主要なアイデアは、$x$ と $y$ を橋渡しするために思考の連鎖 $z\_1, \\dots, z\_n$ を導入することです。ここで各 $z\_i$ は、問題解決に向けた意味のある中間ステップとして機能する一貫した言語シーケンスです（例えば、$z\_i$ は数学QAの中間方程式である可能性があります）。[source: 38] CoTで問題を解決するには、各思考 $z\_i \\sim p\_{\\theta}^{CoT}(z\_i | x, z\_{1\\dots i-1})$ が順次サンプリングされ、次に出力 $y \\sim p\_{\\theta}^{CoT}(y | x, z\_{1\\dots n})$ がサンプリングされます。[source: 39] 実際には、$ [z\_{1\\dots n}, y] \\sim p\_{\\theta}^{CoT}(z\_{1\\dots n}, y | x)$ は連続的な言語シーケンスとしてサンプリングされ、思考の分解（例えば、各 $z\_i$ がフレーズ、文、段落のどれであるか）は曖昧なままです。

[source: 40] CoTを用いた自己整合性（CoT-SC）[36]は、k個の独立同分布（i.i.d.）の思考の連鎖をサンプリングするアンサンブルアプローチです：$[z\_{1\\dots n}^{(i)}, y^{(i)}] \\sim p\_{\\theta}^{CoT}(z\_{1\\dots n}, y | x)$ $(i=1\\dots k)$。[source: 41] 次に、最も頻度の高い出力を返します：$\\arg \\max\_{y} \#{i | y^{(i)} = y}$。CoT-SCはCoTを改善します。なぜなら、同じ問題に対して一般的に異なる思考プロセスが存在し（例えば、同じ定理を証明する異なる方法）、より豊かな思考の集合を探求することによって出力決定がより忠実になる可能性があるからです。[source: 42] しかし、各連鎖内では異なる思考ステップの局所的な探索はなく、「最も頻繁な」ヒューリスティックは出力空間が限られている場合（例えば、多肢選択QA）にのみ適用されます。

**3 Tree of Thoughts: LMを用いた意図的な問題解決**

> [source: 43] 真の問題解決プロセスは、利用可能な情報を繰り返し使用して探索を開始し、それが次にさらなる情報を開示し、最終的に解決策を得る方法が発見されるまで続くことを伴います。
> [source: 44] Newell et al. [21]

[source: 45] 人間の問題解決に関する研究は、人々が組み合わせ的な問題空間、つまりノードが部分的な解決策を表し、枝がそれらを修正する演算子に対応する木を通して探索することを示唆しています [21, 22]。どの枝を取るかは、問題空間をナビゲートし、問題解決者を解決策へと導くのに役立つヒューリスティクスによって決定されます。[source: 46] この視点は、LMを使用して一般的な問題を解決する既存のアプローチの2つの重要な欠点を浮き彫りにします：1) 局所的には、思考プロセス内の異なる継続、つまり木の枝を探求しません。[source: 47] 2) 大域的には、これらの異なる選択肢を評価するのに役立つ計画、先読み、または後戻りのような、人間の問題解決の特徴と思われるヒューリスティック誘導探索を組み込んでいません。

[source: 48] これらの欠点に対処するために、我々は、LMが思考を通じて複数の推論経路を探求することを可能にするパラダイム、Tree of Thoughts（ToT）を導入します（図1(c)）。[source: 49] ToTは、任意の問題を木上の探索としてフレーム化します。ここで各ノードは状態 $s = [x, z\_{1\\dots i}]$ であり、入力とこれまでの思考のシーケンスによる部分的な解決策を表します。[source: 50] ToTの特定のインスタンス化には、4つの質問に答えることが含まれます：

1.  中間プロセスを思考ステップにどのように分解するか。
    [source: 51] 2. 各状態から潜在的な思考をどのように生成するか。
2.  状態をヒューリスティックにどのように評価するか。
3.  どの探索アルゴリズムを使用するか。

-----

**ページ 4**

[source: 52] **1. 思考の分解。** CoTが明示的な分解なしに思考を一貫してサンプリングするのに対し、ToTは問題の特性を活用して中間的な思考ステップを設計および分解します。[source: 53] 表1が示すように、異なる問題に応じて、思考は数語（クロスワード）、1行の方程式（ゲーム・オブ・24）、または執筆計画の段落全体（クリエイティブ・ライティング）になる可能性があります。[source: 54] 一般に、思考はLMが有望で多様なサンプルを生成できる程度に「小さく」（例えば、本全体を生成することは通常、一貫性を保つには「大きすぎる」）、かつLMが問題解決に向けたその見込みを評価できる程度に「大きく」（例えば、1トークンを生成することは通常、評価するには「小さすぎる」）あるべきです。

[source: 55] **2. 思考生成器 $G(p\_{\\theta}, s, k)$。** 木の状態 $s = [x, z\_{1\\dots i}]$ が与えられた場合、次の思考ステップの候補を $k$ 個生成するために2つの戦略を検討します：

(a) [source: 56] CoTプロンプトから独立同分布（i.i.d.）で思考をサンプリングする（クリエイティブ・ライティング、図4）：$z^{(j)} \\sim p\_{\\theta}^{CoT}(z\_{i+1} | s) = p\_{\\theta}^{CoT}(z\_{i+1} | x, z\_{1\\dots i})$ $(j=1\\dots k)$。[source: 57] これは思考空間が豊かである場合（例えば、各思考が段落である場合）により良く機能し、i.i.d.サンプルは多様性をもたらします。
(b) [source: 58] 「提案プロンプト」を使用して思考を順次提案する（ゲーム・オブ・24、図2；クロスワード、図6）：$[z^{(1)}, \\dots, z^{(k)}] \\sim p\_{\\theta}^{propose}(z\_{i+1}^{(1\\dots k)} | s)$。これは思考空間がより制約されている場合（例えば、各思考が単語または行である場合）により良く機能し、同じコンテキストで異なる思考を提案することで重複を避けます。

[source: 59] **3. 状態評価器 $V(p\_{\\theta}, S)$。** 異なる状態のフロンティアが与えられた場合、状態評価器はそれらが問題解決に向けてどれだけ進展するかを評価し、探索アルゴリズムがどの状態を探求し続けるべきか、どの順序で探求すべきかを決定するためのヒューリスティックとして機能します。[source: 60] ヒューリスティクスは探索問題を解決するための標準的なアプローチですが、通常はプログラムされるか（例：DeepBlue [3]）、[source: 61] 学習されます（例：AlphaGo [29]）。我々は、LMを使用して状態について意図的に推論することにより、第3の代替案を提案します。[source: 62] 適用可能な場合、このような意図的なヒューリスティックは、プログラムされたルールよりも柔軟であり、学習モデルよりもサンプル効率が良い可能性があります。[source: 63] 思考生成器と同様に、状態を独立して評価するか、まとめて評価するかの2つの戦略を検討します：

(a) 各状態を独立して評価する：$V(p\_{\\theta}, S)(s) \\sim p\_{\\theta}^{value}(v|s)$ $\\forall s \\in S$。ここで価値プロンプトは状態 $s$ について推論し、スカラー値 $v$ （例：1-10）または分類（例：確実/可能性あり/不可能）を生成し、これをヒューリスティックに値に変換できます。[source: 64] このような評価的推論の根拠は、問題や思考ステップによって異なる可能性があります。[source: 65] この研究では、少数の先読みシミュレーション（例：5, 5, 14が $5+5+14$ で24に到達できることを素早く確認する、または "hot l" が "" に "e" を埋めることで "inn" を意味することを確認する）と常識（例：1 2 3は24に到達するには小さすぎる、または "tzxc" で始まる単語はない）による評価を探求します。[source: 66] 前者は「良い」状態を促進するかもしれませんが、後者は「悪い」状態を排除するのに役立つ可能性があります。[source: 67] このような評価は完璧である必要はなく、意思決定におおよそ役立つだけで十分です。
(b) [source: 68] 状態間で投票する：$V(p\_{\\theta}, S)(s) = I[s=s^*]$。ここで「良い」状態 $s^* \\sim p\_{\\theta}^{vote}(s^\* | S)$ は、投票プロンプトで $S$ 内の異なる状態を意図的に比較することに基づいて投票されます。[source: 69] 問題の成功を直接評価するのが難しい場合（例：文章の一貫性）、代わりに異なる部分的な解決策を比較し、最も有望なものに投票するのが自然です。[source: 70] これは、「ステップワイズ」自己整合性戦略の精神に似ています。すなわち、「どの状態を探求するか」を多肢選択QAとして投げかけ、LMサンプルを使用してそれに投票します。
[source: 71] 両方の戦略について、LMに複数回プロンプトして価値または投票結果を集約し、時間/リソース/コストをより忠実/頑健なヒューリスティクスのためにトレードオフすることができます。

[source: 72]
アルゴリズム 1 ToT-BFS($x, p\_{\\theta}, G, k, V, T, b$)
要求：入力 $x$、LM $p\_{\\theta}$、思考生成器 $G()$ とサイズ制限 $k$、状態評価器 $V()$、ステップ制限 $T$、幅制限 $b$。
$S\_0 \\leftarrow {[x]}$
for $t = 1, \\dots, T$ do
$S'*t \\leftarrow {[s, z] | s \\in S*{t-1}, z \\in G(p\_{\\theta}, s, k)}$
$V\_t \\leftarrow V(p\_{\\theta}, S'*t)$
$S\_t \\leftarrow \\arg \\max*{S \\subset S'*t, |S|=b} \\sum*{s \\in S} V\_t(s)$
end for
return $G(p\_{\\theta}, \\arg \\max\_{s \\in S\_T} V\_T(s), 1)$

アルゴリズム 2 ToT-DFS($s, t, p\_{\\theta}, G, k, V, T, v\_{th}$)
要求：現在の状態 $s$、ステップ $t$、LM $p\_{\\theta}$、思考生成器 $G()$ とサイズ制限 $k$、状態評価器 $V()$、ステップ制限 $T$、閾値 $v\_{th}$。
if $t \> T$ then
記録出力 $G(p\_{\\theta}, s, 1)$
end if
for $s' \\in G(p\_{\\theta}, s, k)$ do ▷ ソートされた候補
if $V(p\_{\\theta}, {s'})(s') \> v\_{th}$ then ▷ 枝刈り
DFS($s', t+1$)
end if
end for

[source: 73] **4. 探索アルゴリズム。** 最後に、ToTフレームワーク内で、木の構造に応じて異なる探索アルゴリズムをプラグアンドプレイできます。我々は比較的単純な2つの探索アルゴリズムを探求し、より高度なもの（例：$A^\*$ [11], MCTS [2]）は将来の研究に残します：

(a) 幅優先探索（BFS）（アルゴリズム1）は、ステップごとに最も有望な $b$ 個の状態の集合を維持します。[source: 74] これは、木の深さが制限され（$T \\le 3$）、初期の思考ステップが評価され、小さな集合（$b \\le 5$）に枝刈りできるゲーム・オブ・24とクリエイティブ・ライティングで使用されます。
(b) [source: 75] 深さ優先探索（DFS）（アルゴリズム2）は、最終出力に到達するまで（$t \> T$）、または状態評価器が現在の $s$ から問題を解決することが不可能であると判断するまで（ある値の閾値 $v\_{th}$ に対して $V(p\_{\\theta}, {s})(s) \\le v\_{th}$）、最も有望な状態を最初に探索します。[source: 76] 後者の場合、$s$ からの部分木は、探索を開発とトレードオフするために枝刈りされます。[source: 77] どちらの場合も、DFSは $s$ の親状態に後戻りして探索を続けます。

[source: 78] 概念的に、ToTはLMを用いた一般的な問題解決の方法としていくつかの利点があります：(1) 一般性。[source: 79] IO、CoT、CoT-SC、および自己改善は、ToTの特殊なケース（すなわち、深さと幅が制限された木；図1）と見なすことができます。(2) モジュール性。[source: 80] ベースとなるLM、および思考の分解、生成、評価、探索手順はすべて独立して変更できます。(3) 適応性。[source: 81] 異なる問題特性、LM能力、およびリソース制約に対応できます。(4) 利便性。[source: 82] 追加のトレーニングは不要で、事前訓練済みのLMだけで十分です。[source: 83] 次のセクションでは、これらの概念的な利点が異なる問題における強力な経験的パフォーマンスにどのようにつながるかを示します。

-----

**ページ 5**

**4 実験**

[source: 84] 我々は、標準的なIOプロンプティングまたはchain-of-thought (CoT) プロンプティングを使用しても、最先端の言語モデルであるGPT-4 [23] でサンプリングする場合でも困難な3つのタスクを提案します。[source: 85] 我々は、思考の木（ToT）における意図的な探索がどのようにしてより良い結果を生み出すか、そしてより重要なことに、探索や計画を必要とする問題を解決するために言語モデルを使用する興味深く有望な新しい方法を示します。

[source: 89] 特に明記されていない限り、サンプリング温度0.7のチャット補完モード $GPT-4^1$ を使用して実験を行います。

**4.1 ゲーム・オブ・24**

[source: 90] ゲーム・オブ・24は数学的推論の挑戦であり、目標は4つの数字と基本的な算術演算子（+-\*/）を使用して24を得ることです。例えば、入力「4 9 10 13」が与えられた場合、解決策の出力は `"(10-4)*(13-9)=24"` となる可能性があります。

[Image 2] 図2：ゲーム・オブ・24におけるToT。(a)思考生成と(b)評価のためにLMにプロンプトされる。

[source: 91]
入力：4 9 10 13
(a) 提案プロンプト
(1例)
入力：4 9 10 13
考えられる次のステップ：
13-9=4
9*4=36
13-4=9
...
思考生成
4*9=36 (残り: 10 13 36)
10-4=6 (残り: 6 9 13)
... さらに行

LM

(b) 価値プロンプト
与えられた数字で24に到達できるか評価する（確実/可能性あり/不可能）
10 14: 10+14=24. 確実
(数例)
10 13 13
思考評価
(13-10)*13=3*13=39
10+13+13=36 これらの大きな数字で24を得る方法はない、不可能

LM

[source: 92] **タスク設定。** https://www.google.com/search?q=%E6%88%91%E3%80%85%E3%81%AF4nums.comからデータを収集しました。ここには人間の解決時間によって簡単なものから難しいものへとソートされた1,362のゲームがあり、テストには比較的難しいゲームのインデックス901-1,000のサブセットを使用します。[source: 93] 各タスクについて、出力が24に等しく、入力された数字をそれぞれ正確に1回使用する有効な方程式である場合、成功と見なします。[source: 94] 100ゲームにわたる成功率を指標として報告します。

**ベースライン。** [source: 95] 5つのインコンテキスト例を含む標準的な入力-出力（IO）プロンプトを使用します。[source: 96] chain-of-thought（CoT）プロンプティングについては、各入出力ペアに3つの中間方程式を追加します。各方程式は残りの2つの数字に対して操作を行います。[source: 97] 例えば、入力「4 9 10 13」が与えられた場合、思考は `"13-9=4 (残り: 4 4 10); 10-4=6 (残り: 4 6);` [source: 98] `4*6=24 (残り: 24)"` となる可能性があります。各ゲームについて、IOとCoTプロンプティングを平均パフォーマンスのために100回サンプリングします。また、100個のCoTサンプルから多数決出力を取るCoT自己整合性ベースラインと、IOサンプルの上で最大10回の反復的な改善アプローチも考慮します。各反復で、出力が間違っている場合、LMは以前のすべての履歴を条件として「間違いを反省し、洗練された回答を生成する」ように指示されます。なお、これは方程式の正しさに関する正解フィードバック信号を使用します。

**ToT設定。** ゲーム・オブ・24をToTにフレーム化するために、思考を3つのステップ、それぞれが中間[source: 99]方程式に分解するのが自然です。図2(a)に示すように、各木のノードで、残りの数字を抽出し、LMにいくつかの可能な次のステップを提案するようにプロンプトします。同じ「提案プロンプト」が3つの思考ステップすべてに使用されますが、入力数字が4つの例は1つしかありません。ToTで幅優先探索（BFS）を実行し、各ステップで最良の $b=5$ 個の候補を保持します。ToTで意図的なBFSを実行するために、図2(b)に示すように、LMに各思考候補を24に到達することに関して「確実/多分/不可能」として評価するようにプロンプトします。目的は、数回の先読み試行内で判定できる正しい部分解を促進し、[source: 100] 「大きすぎる/小さすぎる」という常識に基づいて不可能な部分解を排除し、残りを「多分」として保持することです。[source: 101] 各思考について値を3回サンプリングします。

1 実験は2023年5月5日から16日の間に行われました。

-----

**ページ 6**

[Image 3] 図3：ゲーム・オブ・24 (a) スケーリング分析 & (b) エラー分析。

[source: 102]
表2：ゲーム・オブ・24の結果
| 手法              | 成功率 |
|-------------------|--------|
| IOプロンプト      | 7.3%   |
| CoTプロンプト     | 4.0%   |
| CoT-SC ($k=100$)   | 9.0%   |
| ToT (我々) ($b=1$) | 45%    |
| ToT (我々) ($b=5$) | 74%    |
| IO + Refine ($k=10$) | 27%    |
| IO (100回の最良)  | 33%    |
| CoT (100回の最良) | 49%    |

[source: 103]
図3 (a) 訪問ノード数に対する成功率
(縦軸：成功率、横軸：訪問ノード数)
IO (k回の最良)
CoT (k回の最良)
ToT (b=1..5)

図3 (b) 各ステップで失敗したサンプルの割合
(縦軸：失敗割合、横軸：ステップ [1, 2, 3, 4=Correct])
CoT
ToT (b=5)

**結果。** 表2に示すように、IO、CoT、およびCoT-SCプロンプティング手法はタスクで плохо（悪く）機能し、成功率はそれぞれ7.3％、4.0％、9.0％にすぎません。対照的に、幅 $b=1$ のToTはすでに45％の成功率を達成し、$b=5$ は74％を達成します。また、IO/CoTのオラクル設定も考慮し、$k$ 個のサンプル $(1 \\le k \\le 100)$ の最良を使用して成功率を計算します。[source: 104] IO/CoT（k回の最良）とToTを比較するために、ToTのタスクあたりの訪問ノード数を $b=1\\dots 5$ で計算し、図3(a)の5つの成功率をプロットし、IO/CoT（k回の最良）をバンディットでkノードを訪問するものとして扱います。驚くことではありませんが、CoTはIOよりもスケーリングが良く、100個のCoTサンプルの最良は49％の成功率を達成しますが、ToTでより多くのノードを探索する（$b\>1$）よりもまだはるかに悪いです。

[source: 105] **エラー分析。** 図3(b)は、CoTおよびToTサンプルがどのステップでタスクに失敗したか、すなわち、思考（CoTの場合）またはすべてのb個の思考（ToTの場合）が無効であるか、24に到達することが不可能であるかを分解しています。注目すべきは、CoTサンプルの約60％が、最初のステップ、または同等に最初の3つの単語（例：「4+9」）を生成した後にすでにタスクに失敗していることです。[source: 106] これは、直接的な左から右へのデコーディングの問題点を浮き彫りにしています。

**4.2 クリエイティブ・ライティング**

[source: 107] 次に、入力が4つのランダムな文であり、出力がそれぞれ4つの入力文で終わる4つの段落からなる一貫した文章であるべき、クリエイティブ・ライティングタスクを発明します。このようなタスクは自由形式で探索的であり、創造的思考と高レベルの計画に挑戦します。

**タスク設定。** [source: 108] randomwordgenerator.comからランダムな文をサンプリングして100個の入力を形成し、各入力制約に対する正解の文章はありません。[source: 109] GPT-4がほとんどの場合入力制約に従うことができることがわかったため、文章の一貫性を評価することに焦点を当てます。これは2つの方法で行います：GPT-4のゼロショットプロンプトを使用して1-10のスカラー評点を提供する、または人間の判断を使用して異なる手法からの出力のペアを比較する。[source: 110] 前者については、各タスク出力に対して5つの評点をサンプリングし、それらを平均します。これら5つの評点は通常一貫しており、出力全体で平均して約0.56の標準偏差を持つことがわかりました。[source: 111] 後者については、著者の一部をブラインド研究に採用し、CoT対ToTで生成された文章ペアの一貫性を比較します。ここで、文章の順序は100個の入力に対してランダムに反転されます。

[source: 112] **ベースライン。** タスクの創造的な性質を考えると、IOとCoTの両方のプロンプトはゼロショットです。[source: 113] 前者はLMに入力制約を与えて直接一貫した文章を生成するように促すのに対し、後者はLMに最初に簡単な計画を立ててから文章を書くように促します。つまり、計画が中間的な思考ステップとして機能します。[source: 114] タスクごとに10個のIOおよびCoTサンプルを生成します。[source: 115] また、各タスクのランダムなIOサンプルの上で反復的な改善（$k \\le 5$）手法も考慮します。ここで、LMは入力制約と最後に生成された文章を条件として、文章がすでに「完全に一貫している」かどうかを判断し、そうでなければ洗練されたものを生成します。

[source: 116] **ToT設定。** 深さ2（中間思考ステップは1つのみ）のToTを構築します。LMは最初に $k=5$ 個の計画を生成し、最良のものに投票します（図4）。次に、同様に最良の計画に基づいて $k=5$ 個の文章を生成し、最良のものに投票します。[source: 117] ここでは、各ステップで1つの選択肢のみが保持されるため、幅制限 $b=1$ です。[source: 118] 簡単なゼロショット投票プロンプト（「以下の選択肢を分析し、指示に対してどれが最も有望であるかを結論付けてください」）を使用して、両方のステップで5票をサンプリングします。

-----

**ページ 7**

[Image 4] 図4：ランダムに選択されたクリエイティブ・ライティングタスクにおける意図的な探索のステップ。

[source: 122]
(a) 入力
(b) 計画
計画1
計画2
(c) 投票

[source: 123] 指示：4つの短い段落からなる一貫した文章を書く。各段落の最後の文は次の通りでなければならない：1. 手の上に立つだけであれば、逆立ちは難しくない。2. 宇宙が焼けたステーキの匂いがすることに彼は不意を突かれた。3. 彼女は言い寄ってくる男が好きでないとき、手話を使い始めた。4. あなたを知っている各人は、あなたが誰であるかについて異なる認識を持っている。

[source: 125] 計画1

1.  逆立ちの技術を紹介し説明する。
2.  宇宙飛行士の初めての宇宙での話に切り替える。
3.  女性が望まない注意を避けるために手話を使う状況を描写する。
4.  最後の段落では、誰もが他者に対して異なる認識を持っていることを説明する。

計画2

1.  [source: 126] 逆立ちを挑戦を受け入れるためのメタファーとして言及し、変わった自己啓発本を紹介する。
2.  宇宙飛行士から学んだ予期せぬこと（宇宙の匂いを含む）について議論する。
3.  バーで望まない注意を避けるための女性の巧妙な戦術を描写する。
4.  [source: 127] 自己の異なる認識が自己のアイデンティティをどのように形成するかを熟考する。

計画3-5
...

文章1 文章2
...

投票分析：
各選択肢を詳細に分析する。選択肢1は、要求された最後の文を取り入れているが、段落間の明確なつながりに欠けているように思われる。選択肢2は、要求された最後の文を使用して自己啓発本の内容を提示することにより、興味深い視点を提供する。[source: 128] それは自己改善と挑戦を受け入れるというテーマで段落を結びつけ、一貫した文章にする。[source: 129] 最良の選択肢は2である。

[source: 130] 入力が与えられると、LMは5つの異なる計画をサンプリングし、どの計画が最良かを決定するために5回投票します。[source: 131] 多数決の選択肢が、同じサンプル-投票手順で出力文章を結果的に書くために使用されます。

[Image 5] 図5：クリエイティブ・ライティングの結果。

[source: 132]
(a) GPT-4 一貫性スコア
(箱ひげ図：IO, CoT, ToT, IO+refine, ToT+refine のスコア分布)

(b) 人間による一貫性比較
(棒グラフ：CoT \> ToT, Similar, ToT \> CoT の投票数)
CoT \> ToT: 21
Similar: 38
ToT \> CoT: 41

[source: 133]
表3：ミニクロスワードの結果
| 手法        | 成功率 (%)             |
|-------------|------------------------|
|             | 文字   | 単語   | ゲーム |
| IO          | 38.7   | 14     | 0      |
| CoT         | 40.6   | 15.6   | 1      |
| ToT (我々)  | 78     | 60     | 20     |
| +best state | 82.4   | 67.5   | 35     |
| -prune      | 65.4   | 41.5   | 5      |
| -backtrack  | 54.6   | 20     | 5      |

[source: 119] **結果。** 図5(a)は100タスクにわたる平均GPT-4スコアを示しており、ToT (7.56) はIO (6.19) および CoT (6.93) よりも平均してより一貫した文章を生成すると見なされています。[source: 120] このような自動評価指標はノイズが多いかもしれませんが、図5(b)は、人間が100の文章ペアのうち41でToTをCoTよりも好み、CoTをToTよりも好むのはわずか21であることを示すことで、この発見を確認しています（他の38ペアは「同様に一貫している」と判断されました）。[source: 121] 最後に、反復的改善はこの自然言語タスクでより効果的であり、[source: 135] IOの一貫性スコアを6.19から7.67に、ToTの一貫性スコアを7.56から7.91に改善します。[source: 136] これは、ToTフレームワークにおける思考生成の第3のアプローチと考えることができると我々は信じています。ここでは、新しい思考はi.i.d. [source: 137] または順次生成されるのではなく、古い思考を洗練することから生じます。

-----

**ページ 8**

**4.3 ミニクロスワード**

[source: 137] ゲーム・オブ・24とクリエイティブ・ライティングでは、ToTは比較的浅く、最終出力に到達するために最大でも3つの思考ステップしか必要ありませんでした。[source: 138] ここでは、$5 \\times 5$ のミニクロスワードを、自然言語を含むより困難な探索問題として探求します。[source: 139] 繰り返しになりますが、目標は単にタスクを解決することだけではありません。より一般的なクロスワードは、LMの代わりに大規模な検索を活用する特殊なNLPパイプライン[34]で容易に解決できるからです。[source: 140] むしろ、我々は、自身の思考を探求し、意図的な推論をヒューリスティックとして用いて自身の探索を導く一般的な問題解決者としてのLMの限界を探ることを目指します。

[source: 141] **タスク設定。** 我々はGooBixからデータを収集しました。ここには$5 \\times 5$ミニクロスワードの156ゲームが含まれています。[source: 142] 隣接するゲームが類似の手がかりを含むことを観察したため、テストにはインデックス1, 6, ..., 91, 96の20ゲームを使用し、プロンプティングにはゲーム136, 141, 146, 151, 156を使用します。[source: 143] 各タスクについて、入力は5つの水平の手がかりと5つの垂直の手がかりを記述し、出力はクロスワードを解決するための$5 \\times 5 = 25$文字の盤面であるべきです。[source: 144] 評価のために、3つのレベルの成功を考慮します：正しい文字の割合（ゲームあたり25）、単語の割合（ゲームあたり10）、およびゲームの割合。

[source: 145] **ベースライン。** IOプロンプトで5つの入力-出力例を提供し、CoTプロンプトではさらにh1..5、次にv1..5の順で中間単語を含めます。[source: 146] 各プロンプトを10サンプル実行し、結果を平均します。

**ToT設定。** [source: 147] 深さ優先探索（アルゴリズム2）を活用します。これは、状態がもはや有望でなくなるまで最も有望な後続の単語の手がかりを探求し続け、次に親状態に後戻りして代替の思考を探求します。[source: 148] 探索を実行可能にするために、後続の思考は、埋められた単語や文字を変更しないように制約されます。これにより、ToTは最大でも10の中間ステップを持ちます。[source: 149] 思考生成のために、各状態で既存のすべての思考（例：図6(a)の状態の"h2.motor; h1.tasks"）を残りの手がかりの文字制約（例："v1.To heap: tm\_\_\_;..."）に変換し、提案プロンプトを5回プロンプトして、次に埋める単語の場所と内容の候補を考え出します。[source: 150] 重要なことに、LMに異なる思考に対する信頼度レベルを与えるようにプロンプトし、[source: 154] これらを提案全体で集約して、探索する次の思考のソート済みリストを取得します（図6(a)）。[source: 155] 状態評価のために、同様に各状態を残りの手がかりの文字制約に変換し、次に各手がかりについて、制約を与えられた場合に埋めることが可能かどうかを評価します。[source: 156] 残りの手がかりのいずれかが埋めることが「不可能」と見なされた場合（例："v1. To heap: tm\_s\_"）、その状態の部分木の探索は枝刈りされ、DFSはその親に後戻りして次の有望な思考を探求します。[source: 157] DFS探索ステップを100に制限し、最も深く探索された状態（複数ある場合は最初に探索されたもの）を最終出力として単純にレンダリングします。

[Image 6] 図6：ミニクロスワードにおいて、(a) 思考がどのように提案され、深さ優先探索（DFS）のための優先度付きキューに集約されるか、(b) 残りの各単語の手がかりを埋める可能性に基づいて状態がどのように評価され、LMによって埋めることが不可能と見なされた残りの手がかりがあれば枝刈りされるか。

[source: 151]
(a) 思考提案
入力の手がかり: h1.tasks, h2.motor, ...
現在の状態: h1.tasks, h2.motor が埋まっている盤面
提案:
h4. salon (確実)
v5. srdry (低い)
v3. string (高い)
...
集約 & ソート (DFS順序):

1.  h4.salon
2.  v3.string
    ...

(b) 状態評価 (各残りの手がかりについて)
状態: h1.tasks, h2.motor, h4.salon が埋まっている
残りの手がかり:
v3. Pretentious, flowery: s\_r\_n\_ → maybe (可能性あり)
v1. To heap: tm\_s\_ → impossible (不可能) → この部分木は枝刈り
v5. Desiccator, more dry: sr\_n\_ → maybe (可能性あり)

[source: 152] [source: 153] その後、DFSは親状態に後戻りし、次の有望な手がかりの思考を探求します。

[source: 158] **結果。** 表3に示すように、IOおよびCoTプロンプティング手法は単語レベルの成功率が16％未満と低迷していますが、ToTはすべての指標を大幅に改善し、単語レベルの成功率60％を達成し、20ゲーム中4ゲームを解決します。[source: 159] IOとCoTには異なる手がかりを試したり、決定を変更したり、後戻りしたりするメカニズムがないことを考えると、このような改善は驚くべきことではありません。

[source: 160] **オラクルおよびアブレーション研究。** タスクごとに（ヒューリスティックに決定された最良の状態ではなく）オラクルの最良DFS状態から出力する場合、ToTのパフォーマンスはさらに高く、実際には$7/20$ゲームを解決します（表3、「+best state」）。これは、我々の単純な出力ヒューリスティックが容易に改善できることを示しています。[source: 161] 興味深いことに、クロスワードゲームが実際に解決された場合でも、状態評価器が一部の単語を「不可能」と見なし、枝刈りすることがあります。これはおそらく、$5\\times 5$クロスワードが設計上、GPT-4が認識できないいくつかの稀なまたは廃れた単語を含んでいるためです²。[source: 162] 枝刈りヒューリスティックとしての状態評価が不完全であることを考えると、枝刈りを除去するアブレーションも探求し、パフォーマンスが一般的に悪化することを発見しました（表3、「-prune」）。[source: 163] しかし、実際には$4/20$ゲームの正解を見つけることができました（ただし、ヒューリスティックを介して出力されたのは1つのみ）。そのうち3つはToT+枝刈りが100ステップ以内に解決できなかったゲームです。[source: 164] したがって、この場合のDFS枝刈りのためのより良いヒューリスティックが問題解決にとって重要です。[source: 165] 最後に、最大20ステップで最も有望な手がかりを埋め続け、上書きを許可するアブレーションを実行することにより、後戻りの重要性を確認します。[source: 166] これは、幅制限$b=1$の「貪欲な」BFS探索に似ており、単語レベルの成功率がわずか20％と低迷します（表3、「-backtrack」）。

² 例えば、「agend」は「agendum」の廃れた形ですが、GPT-4はそれを「agenda」のタイプミスと見なします。[source: 173] 外部検索やウェブインタラクションは、知識の不確実性の下での問題解決のためにLMを拡張する可能性があります。

-----

**ページ 9**

**5 関連研究**

**計画と意思決定。** [source: 167] スマートな計画と意思決定は、事前に定義された目標を達成するために重要です。[source: 168] LMは膨大な世界の知識と人間の例で訓練されているため、問題設定と環境状態を条件として合理的な計画を提案することを可能にする豊かな常識をすでに吸収していることが知られています [12, 42, 37, 13, 35, 41, 40]。[source: 169] 我々が提案するToTアプローチは、各問題解決ステップで複数の潜在的に実行可能な計画を同時に考慮し、最も有望なもので進むことにより、既存の計画定式化を拡張します。[source: 170] 思考サンプリングと価値フィードバックの統合は、計画と意思決定メカニズムを有機的に統合し、解決策ツリー内の効果的な探索を可能にします。[source: 171] 一方、従来の意思決定手順は通常、強化学習のように専用の報酬モデルとポリシーモデルの訓練を必要としますが（例えばCHAI [33]）、我々はLM自体を使用して意思決定のための価値推定を提供します。[source: 172] RAP [9] は、言語モデルの推論をその内部世界モデルを用いた計画として扱い、ToTに類似したMCTSベースの手法を提案する同時期の研究です。[source: 174] [source: 175] しかし、そのタスクは我々のものよりも単純であり、そのフレームワークには異なる木探索アルゴリズムを組み込むためのモジュール性が欠けています。

**自己反省。** [source: 176] LLMを使用して自身の予測の実行可能性を評価することは、問題解決においてますます重要な手順になっています。[source: 177] [28, 20, 24] は「自己反省」メカニズムを導入しました。ここでは、LMが生成候補にフィードバックを提供します。[source: 178] [4] は、コード実行結果に基づいてLM自体によって生成されたフィードバックメッセージを注入することにより、LMのコード生成精度を向上させます。[source: 179] 同様に、[17] も、コンピュータ操作タスクを解決する際に取るべき次のアクションを決定するために、アクションと状態に対する「批評家」またはレビュー段階を導入しています。[source: 180] 我々の研究に非常に関連するもう一つの最近の研究は、「自己評価誘導デコーディング」[39] です。[source: 181] 我々の手法と同様に、自己評価デコーディングも、確率的ビームサーチデコーディングからサンプリングされた葉を持つ木探索手順に従います。これらの葉は、慎重に準備された自己評価プロンプトを用いてLLM自体によって評価されます。[source: 182] しかし、彼らのアプローチは、思考をコードとして表現するPAL定式化[8]を使用しており、我々がこの論文で検討するクリエイティブ・ライティングのような挑戦的なタスクに取り組むことを困難にしています。[source: 183] したがって、我々のTree-of-Thought定式化はより多用途であり、GPT-4が標準的なプロンプトでは非常に低い精度しか達成できない挑戦的なタスクを扱います。

**プログラム誘導LLM生成。** [source: 184] 我々の提案は、体系的な手順[14, 44, 6, 43]または記号的なプログラムガイダンスを用いてLMの振る舞いを整理する最近の進歩にも関連しています。[source: 185] 例えば、Schlag et al. [27] は、質問応答のような問題を段階的に解決するのを助けるために、アルゴリズム的な探索手順にLMを埋め込みます。ここでは、探索木は回答を提供する可能性のある関連段落によって拡張されます。[source: 186] しかし、このアプローチは、木がLM自身の思考ではなく外部の段落をサンプリングすることによって拡張され、反省や投票のステップがない点で我々のものとは異なります。[source: 187] 別のアプローチであるLLM+P [18] はさらに一歩進んで、実際の計画プロセスを古典的なプランナーに委任します。

-----

**ページ 10**

**古典的探索手法。** [source: 188] 最後に重要なこととして、我々のアプローチは、問題解決のための古典的な探索手法の現代的な表現として扱われる可能性があります。[source: 189] 例えば、各探索ノードでのヒューリスティックがLMの自己評価によって提供される、A\* [10] のようなヒューリスティック探索アルゴリズムと見なすことができます。[source: 190] この観点から、我々の手法はNeuroLogic A*esqueデコーディング [19] にも関連しています。これは、$A^*$ 探索に触発されていますが、ビームサーチまたはトップkサンプリングデコーディングを改善するためにLMにとって効率的な先読みヒューリスティックを導入しています。[source: 191] しかし、この手法は文生成タスクに限定されていますが、我々のフレームワークは価値フィードバックによって保護された複雑な多段階の問題解決のために設計されています。

**6 議論**

**限界と将来の方向性。** [source: 192] ToTのような意図的な探索は、GPT-4がすでに優れている多くの既存のタスクには必要ないかもしれません（付録B.1参照）。そして、最初のステップとして、この研究はGPT-4に挑戦し（いくつかのGPT-3.5実験結果については付録B.2参照）、LMと統合されたより良い探索と計画能力を求める、比較的単純な3つのタスクのみを探求しています。[source: 193] しかし、我々がLMをより現実世界の意思決定アプリケーション（例：コーディング、データ分析、ロボティクスなど）に展開し始めるにつれて、より複雑なタスクが出現し、これらの研究課題を研究するための新しい機会を提供する可能性があります。[source: 194] また、ToTのような探索手法は、タスクパフォーマンスを向上させるためにサンプリング手法よりも多くのリソース（例：GPT-4 APIコスト）を必要としますが、ToTのモジュール的な柔軟性により、ユーザーはそのようなパフォーマンスとコストのトレードオフをカスタマイズでき、進行中のオープンソースの取り組み[32]は近い将来そのようなコストを容易に削減するはずです。[source: 195] コストと効率に関する詳細は付録B.3にあります。最後に、この研究は既製のLMを使用することに焦点を当てており、ToTスタイルの高レベルの反事実的な意思決定（例：次のトークンを予測するのではなく、次の段落の潜在的な選択肢について熟考する）を使用してLMをファインチューニングすることは、LMの問題解決能力を高める機会を提供する可能性があります。

**結論。** [source: 196] LMの連想的な「システム1」は、問題の解決策への可能な経路の木を探索することに基づいた「システム2」によって有益に補強され得ます。[source: 197] Tree of Thoughtsフレームワークは、問題解決に関する古典的な洞察を現代のLMのための実行可能な手法に変換する方法を提供します。[source: 198] 同時に、LMはこれらの古典的な手法の弱点に対処し、クリエイティブ・ライティングのような容易に形式化できない複雑な問題を解決する方法を提供します。[source: 199] 我々は、LMと古典的なAIアプローチとのこの交差点を、エキサイティングな方向性と見ています。

**広範な影響**

[source: 200] ToTは、LMがより自律的かつインテリジェントに意思決定を行い、問題を解決することを可能にするフレームワークです。[source: 201] 現在のタスクは推論と探索問題に限定されていますが、外部環境や人間との相互作用を伴う将来のアプリケーションは、潜在的な危険をもたらす可能性があります。例えば、[source: 202] LMの有害な使用を助長するなどです。一方で、ToTはモデルの意思決定の解釈可能性と人間のアラインメントの機会も改善します。なぜなら、結果として得られる表現は、暗黙的で低レベルのトークン値ではなく、読み取り可能で高レベルの言語推論だからです。

**謝辞**

[source: 203] SYとKNは、Oracle Collaborative Research awardおよびNational Science Foundation Grant No. 2239363からの支援に謝意を表します。本資料で表明されたいかなる意見、発見、結論、または推奨も、著者（ら）のものであり、必ずしもNational Science Foundationの見解を反映するものではありません。[source: 204] SYはまた、プリンストンからのHarold W. Dodds Fellowshipによって支援されています。

**参考文献**

[1] [source: 205] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. [source: 206] 言語モデルは少数ショット学習者である。Advances in neural information processing systems, 33:1877-1901, 2020.

[2] [source: 207] C. Browne, E. J. Powley, D. Whitehouse, S. M. M. Lucas, P. I. Cowling, P. Rohlfshagen, S. Tavener, D. P. Liebana, S. Samothrakis, and S. Colton. モンテカルロ木探索手法の調査。IEEE Transactions on Computational Intelligence and Al in Games, 4:1-43, 2012.

[3] [source: 208] M. Campbell, A. J. Hoane Jr, and F.-h. Hsu. ディープ・ブルー。Artificial intelligence, 134(1-2):57-83, 2002.

[4] [source: 209] X. Chen, M. Lin, N. Schärli, and D. Zhou. 大規模言語モデルに自己デバッグを教える、2023。

[5] [source: 210] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Pathwaysによる言語モデリングのスケーリング。arXiv preprint arXiv:2204.02311, 2022.

[6] [source: 211] A. Creswell and M. Shanahan. 大規模言語モデルを用いた忠実な推論。arXiv preprint arXiv:2208.14271, 2022.

[7] [source: 212] N. D. Daw, Y. Niv, and P. Dayan. [source: 213] 行動制御のための前頭前野システムと背外側線条体システム間の不確実性に基づく競争。Nature neuroscience, 8(12):1704-1711, 2005.

[8] [source: 214] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. Pal: プログラム支援言語モデル、2023。

[9] [source: 215] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu. 言語モデルを用いた推論は世界モデルを用いた計画である。arXiv preprint arXiv: 2305.14992, 2023.

[10] [source: 216] P. E. Hart, N. J. Nilsson, and B. Raphael. [source: 217] 最小コスト経路のヒューリスティック決定のための形式的基礎。IEEE Transactions on Systems Science and Cybernetics, 4(2):100-107, 1968. doi: 10.1109/TSSC.1968.300136.

[11] [source: 218] P. E. Hart, N. J. Nilsson, and B. Raphael. [source: 219] 最小コスト経路のヒューリスティック決定のための形式的基礎。IEEE transactions on Systems Science and Cybernetics, 4(2):100-107, 1968.

[12] [source: 220] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. ゼロショットプランナーとしての言語モデル：実体化エージェントのための実行可能な知識の抽出、2022。

[13] [source: 221] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, et al. 内的独白：言語モデルを用いた計画による実体化推論。arXiv preprint arXiv:2207.05608, 2022.

-----

**ページ 11**

[14] [source: 222] J. Jung, L. Qin, S. Welleck, F. Brahman, C. Bhagavatula, R. L. Bras, and Y. Choi. [source: 223] Maieutic prompting: 再帰的説明を用いた論理的に一貫した推論。arXiv preprint arXiv:2205.11822, 2022.

[15] [source: 224] D. Kahneman. ファスト＆スロー。Macmillan, 2011.

[16] [source: 225] D. Kahneman, S. Frederick, et al. 代表性の再訪：直感的判断における属性置換。Heuristics and biases: The psychology of intuitive judgment, 49(49-81):74, 2002.

[17] [source: 226] G. Kim, P. Baldi, and S. McAleer. 言語モデルはコンピュータタスクを解決できる、2023。

[18] [source: 227] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone. Llm+p: 最適な計画能力で大規模言語モデルを強化する、2023。

[19] [source: 228] X. Lu, S. Welleck, P. West, L. Jiang, J. Kasai, D. Khashabi, R. L. Bras, L. Qin, Y. Yu, R. Zellers, N. A. Smith, and Y. Choi. Neurologic a\*esque decoding: 先読みヒューリスティックを用いた制約付きテキスト生成。In North American Chapter of the Association for Computational Linguistics, 2021.

[20] [source: 229] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, S. Welleck, B. P. Majumder, S. Gupta, A. Yazdanbakhsh, and P. Clark. Self-refine: 自己フィードバックを用いた反復的改善、2023。

[21] [source: 230] A. Newell, J. C. Shaw, and H. A. Simon. 一般的問題解決プログラムに関する報告。In IFIP congress, volume 256, page 64. Pittsburgh, PA, 1959.

[22] [source: 231] A. Newell. H. A. Simon, et al. 人間の問題解決。Prentice-Hall, 1972.

[23] [source: 232] OpenAI. GPT-4テクニカルレポート。ArXiv, abs/2303.08774, 2023.

[24] [source: 233] D. Paul, M. Ismayilzada, M. Peyrard, B. Borges, A. Bosselut, R. West, and B. Faltings. Refiner: 中間表現に対する推論フィードバック、2023。

[25] [source: 234] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. 生成的事前訓練による言語理解の改善。OpenAI blog, 2018.

[26] [source: 235] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. 言語モデルは教師なしマルチタスク学習者である。OpenAI blog, 1(8):9, 2019.

[27] [source: 236] I. Schlag, S. Sukhbaatar, A. Celikyilmaz, W. tau Yih, J. Weston, J. Schmidhuber, and X. Li. 大規模言語モデルプログラム、2023。

[28] [source: 237] N. Shinn, B. Labash, and A. Gopinath. Reflexion: 動的メモリと自己反省を持つ自律エージェント、2023。

[29] [source: 238] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al. 人間の知識なしで囲碁ゲームをマスターする。nature, 550 (7676):354-359, 2017.

[30] [source: 239] S. A. Sloman. 2つの推論システムに対する経験的根拠。Psychological bulletin, 119(1): 3, 1996.

[31] [source: 240] K. E. Stanovich. 誰が合理的か？推論における個人差の研究。Psychology Press, 1999.

[32] [source: 241] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: オープンで効率的な基盤言語モデル。[source: 242] arXiv preprint arXiv: 2302.13971, 2023.

[33] [source: 243] S. Verma, J. Fu, S. Yang, and S. Levine. Chai: オフライン強化学習を用いたタスク指向対話のためのチャットボットAI。[source: 244] In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4471-4491, 2022.

-----

**ページ 12**

[34] [source: 245] E. Wallace, N. Tomlin, A. Xu, K. Yang, E. Pathak, M. Ginsberg, and D. Klein. 自動クロスワード解決。[source: 246] arXiv preprint arXiv:2205.09665, 2022.

[35] [source: 247] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K.-W. Lee, and E.-P. Lim. Plan-and-solveプロンプティング：大規模言語モデルによるゼロショットchain-of-thought推論の改善、2023。

[36] [source: 248] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. 自己整合性は言語モデルにおけるchain of thought推論を改善する。arXiv preprint arXiv:2203.11171, 2022.

[37] [source: 249] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang. 記述、説明、計画、選択：大規模言語モデルを用いたインタラクティブな計画がオープンワールドマルチタスクエージェントを可能にする、2023。

[38] [source: 250] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thoughtプロンプティングは大規模言語モデルにおける推論を引き出す。arXiv preprint arXiv:2201.11903, 2022.

[39] [source: 251] Y. Xie, K. Kawaguchi, Y. Zhao, X. Zhao, M.-Y. Kan, J. He, and Q. Xie. 分解は自己評価誘導デコーディングを通じて推論を強化する、2023。

[40] [source: 252] S. Yang, O. Nachum, Y. Du, J. Wei, P. Abbeel, and D. Schuurmans. 意思決定のための基盤モデル：問題、手法、および機会、2023。

[41] [source: 253] S. Yao, J. Zhao, D. Yu. N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: 言語モデルにおける推論と行動の相乗効果。[source: 254] arXiv preprint arXiv:2210.03629, 2022.

[42] [source: 255] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan. コード生成のための大規模言語モデルを用いた計画。In The Eleventh International Conference on Learning Representations, 2023. URL [https://openreview.net/forum?id=Lr8c00tYbfL](https://www.google.com/search?q=https://openreview.net/forum%3Fid%3DLr8c00tYbfL).
[43] [source: 256] D. Zhou, N. Schärli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet, Q. Le, et al. [source: 257] Least-to-mostプロンプティングは大規模言語モデルにおける複雑な推論を可能にする。arXiv preprint arXiv:2205.10625, 2022.

[44] [source: 258] X. Zhu, J. Wang, L. Zhang, Y. Zhang, R. Gan, J. Zhang, and Y. Yang. 協調的推論誘導言語モデルによる数学文章題の解決。arXiv preprint arXiv:2210.16257, 2022.

-----

**ページ 13**

**付録 A コード、プロンプト、軌跡**

[source: 259] 全てのコードは [https://github.com/princeton-nlp/tree-of-thought-11m](https://www.google.com/search?q=https://github.com/princeton-nlp/tree-of-thought-11m) で利用可能です。

全てのプロンプトは [https://github.com/princeton-nlp/tree-of-thought-1lm/tree/master/src/tot/prompts](https://www.google.com/search?q=https://github.com/princeton-nlp/tree-of-thought-1lm/tree/master/src/tot/prompts) で利用可能です。

軌跡は [https://github.com/princeton-nlp/tree-of-thought-llm/tree/master/logs](https://github.com/princeton-nlp/tree-of-thought-llm/tree/master/logs) で利用可能です。

**付録 B 追加実験結果**

[source: 260] 言語モデルの能力フロンティアを探求し拡張するという動機から、本論文の実験は最先端の言語モデル（GPT-4）と、それに挑戦するために考案された3つの困難なタスクを用いた設定に焦点を当ててきました。[source: 261] ここでは、より弱いLLMまたはより簡単なタスクを用いた追加実験を報告し、コストと効率について議論します。

[source: 262]
表4：ゼロショットToTとGPT-4を用いた新しいタスクの結果
|             | GSM8K | StrategyQA |
|-------------|-------|------------|
|             | GPT-4 | GPT-4      |
| IO          | 51    | 73         |
| CoT         | 86    | 82         |
| ToT         | 90    | 83         |

[source: 263]
表5：GPT-4 vs GPT-3.5でのゲーム・オブ・24の結果
|       | GPT-4 | GPT-3.5 |
|-------|-------|---------|
| IO    | 7.3%  | 6%      |
| CoT   | 4.0%  | 3%      |
| ToT   | 74%   | 19%     |

[source: 264]
表6：GPT-4 vs GPT-3.5でのクリエイティブ・ライティングの結果
|       | GPT-4 | GPT-3.5 |
|-------|-------|---------|
| IO    | 6.19  | 4.47    |
| CoT   | 6.93  | 5.16    |
| ToT   | 7.56  | 6.62    |

**B.1 新しいタスク（GSM8k、StrategyQA）へのゼロショットToTの拡張**

[source: 264] より一般的なNLPタスクはGPT-4にとっては簡単すぎ、ToTを必要としないかもしれませんが（だからこそ我々はより困難な新しいタスクを検討しました）、新しいタスクにToTを適用することは簡単であると我々は信じています。[source: 265] 例えば、クリエイティブ・ライティングと同様の単純で汎用的なゼロショットToT-BFS（5つの問題解決戦略をサンプリングしてから最良のものに投票する；次に最良の戦略に基づいて5つの解決策をサンプリングしてから最良のものに投票する）を、数行の追加コードでGSM8KとStrategyQAに実装しました：

```python
# define the answer format of new tasks
gsm8k_format = '"the answer is n" where n is a number'
strategyqa_format = 'either "the answer is yes" or "the answer is no"'

# define zero-shot io prompting
standard_prompt = 'Answer the following question with {format}: {input}'

# define thought format for zero-shot cot and zero-shot tot
cot_prompt = '''
[source: 266] Answer the following question: {input}

Make a strategy then write.
[source: 267] Your output should be of the following format:

Strategy:
Your strategy about how to answer the question.

Answer:
Your answer to the question.
[source: 268] It should end with {format}.
'''

# define zero-shot voting used for zero-shot tot
vote_prompt = '''Given an instruction and several choices,
decide which choice is most promising.
[source: 269] Analyze each choice in detail, then conclude in the last line
"The best choice is {s}", where s the integer id of the choice.'''
[source: 270] # (原文ママ)
```

-----

**ページ 14**

[source: 271] 我々は、100個のランダムなGSM8Kテスト質問とStrategyQA開発質問のサブセットで評価しました。[source: 272] 表4に示すように、そして予想通り、ToTは両方のタスクでCoTを改善します（ただし、GPT-4 + CoTがすでにそのようなタスクで非常に優れており、StrategyQAのボトルネックが推論ではなく外部知識であることを考えると、わずかに改善するだけです）。[source: 273] 計算コストを考慮すると、従来のNLPタスクにはより小さなLLM + ToTを試す方が適しており、GPT-4 + CoTの推論に挑戦する困難なタスクにはGPT-4 + ToTを試す方が適しています。

**B.2 新しいLMS（GPT-3.5）への拡張**

[source: 274] ToTが他のLLMでどのように機能するかを理解するために、GPT-3.5-turboでもクリエイティブ・ライティング（表6）とゲーム・オブ・24（表5）を実行しました。[source: 275] 両方のタスクで、GPT-3.5に対しても "$ToT \> CoT \> IO$" が真であることが確認されました。クリエイティブ・ライティングでは、GPT-3.5+ToTがGPT-4+IOを上回り、GPT-4+CoTと同様であることがわかりました。これは、ToTがより弱い言語モデルでもうまく機能する可能性を示唆しています。[source: 276] ゲーム・オブ・24では（機能させるために1ショット提案プロンプトを3ショットに変更しました）、GPT-3.5+ToTの19%はGPT-4+ToTの74%よりもはるかに悪いです。[source: 277] 生成と評価の重要性をさらに理解するために、GPT-4生成+GPT-3.5評価（64%）とGPT-3.5生成+GPT-4評価（31%）を実行しました。[source: 278] これは、ゲームのボトルネックが思考生成であり、異なる生成/評価言語モデルがコストを削減しつつもまともな結果を達成できる可能性があることを示唆しています。

**B.3 コストと効率**

[source: 279] ToTの実行は、IOまたはCoTプロンプティングよりも大幅に多くの計算を必要とします。[source: 280] 例えば、ゲーム・オブ・24（以下の表7）では、ToTで問題を解決するには5.5kの補完トークンが必要であり、これは100回のCoT試行（6.7kトークン）に近い値です。[source: 281] しかし、ToTのパフォーマンスは、100回の独立したCoT試行の最良よりも優れています。

[source: 282]
表7：ゲーム・オブ・24のコスト分析
|                   | 生成/プロンプト トークン | ケースあたりのコスト | 成功率 |
|-------------------|--------------------------|--------------------|--------|
| IO (100回の最良)  | 1.8k/1.0k                | $0.13             | 33%    |
| CoT (100回の最良) | 6.7k/2.2k                | $0.47             | 49%    |
| ToT               | 5.5k/1.4k                | $0.74             | 74%    |

[source: 283] クリエイティブ・ライティング（以下の表8）では、ToTは約5倍の補完トークンと費用がかかることがわかりました。これは、$b=5$ であり、ほとんどのトークンが生成された文章であるため直感的です。

[source: 284]
表8：クリエイティブ・ライティングのコスト分析 (※ 表7のキャプションは原文誤りと思われます)
|                    | 生成/プロンプト トークン | ケースあたりのコスト |
|--------------------|--------------------------|--------------------|
| IO                 | 0.9k/0.4k                | $0.06             |
| CoT                | 0.9k/0.4k                | $0.07             |
| ToT                | 4k/2.9k                  | $0.32             |

[source: 285] したがって、ゲーム・オブ・24とクリエイティブ・ライティングの主要なToT実験を完了するには、約 $0.74 \\times 100 + 0.32 \\times 100 = 106$ ドルかかりました。[source: 286] クロスワードのDFS実験も100ドル以内のはずです。一般に、ToTのコストと効率は、使用されるプロンプトと探索アルゴリズムに大きく依存し、CoTよりも5〜100倍多くの生成トークンを必要とする可能性があります。[source: 287] いくつか実行可能な洞察：

  * [source: 287] CoTが苦戦する、意図的な推論を必要とするタスクにToTを使用することをお勧めします。
  * [source: 288] ToTの柔軟性により、パフォーマンスとコストのトレードオフが可能です。例えば、BFSのビームサイズや投票数を変更する、少数ショット vs ゼロショットプロンプティング、GPT-3.5 vs GPT-4などです。いくつかのリソース制約やパフォーマンス目標に基づいて設定を構成できます。
  * [source: 289] 効率を改善する余地はたくさんあります。例えば、BFSは解決策が見つかったときに早期停止したり、いくつかの思考が「不可能」なときにビームサイズを削減したりできます。
  * [source: 290] モデルがより強力な知能を達成するためには、確かにより多くの計算が必要であると我々は信じており、長期的には（オープンソース）LMがはるかに安価で効率的になるため、これが妨げになる問題になるべきではありません。[source: 291] また、思考生成および/または評価のためにLMをより良く訓練/ファインチューニングする方法も、素晴らしい方向性です。

-----

以上で、提供されたPDFファイルの全翻訳が完了しました。